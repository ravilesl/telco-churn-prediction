{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44afdabc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#                 PASO 1: INSTALACIÓN DE LIBRERÍAS Y DESCARGA DE DATOS\n",
    "# ==============================================================================\n",
    "# Usamos -q para una instalación silenciosa.\n",
    "!pip install -q pandas numpy scikit-learn xgboost matplotlib seaborn imbalanced-learn\n",
    "\n",
    "# Descargamos el dataset directamente desde la URL y lo guardamos en la ruta esperada.\n",
    "!wget -P data/raw/ https://raw.githubusercontent.com/IBM/telco-customer-churn-extra-datasets/master/Telco-Customer-Churn.csv\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#                 PASO 2: CÓDIGO DEL PIPELINE COMPLETO\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# IMPORTS NECESARIOS\n",
    "# ------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from typing import Optional\n",
    "\n",
    "# Preprocesamiento y modelado\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "# Evaluación\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MÓDULO utils/data_loader.py (unificado)\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_raw_data(file_path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Carga los datos crudos desde un archivo CSV.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"Datos cargados exitosamente.\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no fue encontrado en {file_path}\")\n",
    "        return None\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MÓDULO features/feature_engineering.py (unificado)\n",
    "# ------------------------------------------------------------------------------\n",
    "def preprocess_and_split(df: pd.DataFrame, target: str):\n",
    "    \"\"\"\n",
    "    Procesa el dataframe y prepara los datos para el pipeline de imblearn.\n",
    "    \n",
    "    Returns:\n",
    "        X (pd.DataFrame): DataFrame con las características procesadas.\n",
    "        y (pd.Series): Serie con la variable objetivo.\n",
    "        preprocessor (ColumnTransformer): El objeto preprocesador ajustado.\n",
    "    \"\"\"\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df[target] = df[target].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns\n",
    "    numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.drop(target)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    X = df.drop(columns=[target], axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    return X, y, preprocessor\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MÓDULO models/model_factory.py (unificado)\n",
    "# ------------------------------------------------------------------------------\n",
    "class ModelFactory:\n",
    "    \"\"\"Clase que crea instancias de modelos y las prepara para la optimización.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_model_base(model_name: str):\n",
    "        \"\"\"Crea solo la instancia base de un modelo.\"\"\"\n",
    "        if model_name == 'xgboost':\n",
    "            return xgb.XGBClassifier()\n",
    "        \n",
    "        raise ValueError(f\"Modelo '{model_name}' no soportado.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MÓDULO evaluation/metrics_evaluator.py (unificado)\n",
    "# ------------------------------------------------------------------------------\n",
    "def evaluate_model(y_true, y_pred, model_name: str):\n",
    "    \"\"\"Evalúa las predicciones del modelo y genera un reporte.\"\"\"\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n--- Reporte de Evaluación para {model_name} ---\")\n",
    "    print(report)\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
    "    plt.title(f'Matriz de Confusión para {model_name}')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# MÓDULO pipelines/training_pipeline.py (unificado)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Umbrales para clasificar el tipo de desbalance\n",
    "HIGH_IMBALANCE_THRESHOLD = 0.10\n",
    "SEMI_IMBALANCE_THRESHOLD = 0.30\n",
    "\n",
    "def detect_class_imbalance(y):\n",
    "    \"\"\"\n",
    "    Detecta si el dataset está desbalanceado y retorna la métrica de scoring,\n",
    "    la grilla de parámetros iniciales y las técnicas de balanceo a probar.\n",
    "    \"\"\"\n",
    "    class_counts = pd.Series(y).value_counts(normalize=True)\n",
    "    minority_class_ratio = class_counts.min()\n",
    "    print(f\"\\nProporción de la clase minoritaria: {minority_class_ratio:.2%}\")\n",
    "\n",
    "    balancing_techniques = {'no_balancing': None}\n",
    "    scoring_metric = 'accuracy'\n",
    "    initial_params_grid = {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.05, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "    }\n",
    "\n",
    "    if minority_class_ratio < HIGH_IMBALANCE_THRESHOLD:\n",
    "        print(\"Dataset altamente desbalanceado. Usando f1-score y técnicas de sobremuestreo agresivas.\")\n",
    "        scoring_metric = 'f1'\n",
    "        scale_pos_weight_value = class_counts[0] / class_counts[1]\n",
    "        initial_params_grid['model__scale_pos_weight'] = [scale_pos_weight_value]\n",
    "        balancing_techniques.update({\n",
    "            'smote': SMOTE(random_state=42),\n",
    "            'adasyn': ADASYN(random_state=42)\n",
    "        })\n",
    "    elif HIGH_IMBALANCE_THRESHOLD <= minority_class_ratio < SEMI_IMBALANCE_THRESHOLD:\n",
    "        print(\"Dataset semidesbalanceado. Usando f1-score y técnicas de muestreo aleatorio.\")\n",
    "        scoring_metric = 'f1'\n",
    "        balancing_techniques.update({\n",
    "            'random_oversampler': RandomOverSampler(random_state=42),\n",
    "            'random_undersampler': RandomUnderSampler(random_state=42)\n",
    "        })\n",
    "    else:\n",
    "        print(\"Dataset balanceado. Usando accuracy y no se aplican técnicas de balanceo.\")\n",
    "    \n",
    "    return scoring_metric, initial_params_grid, balancing_techniques\n",
    "\n",
    "def run_training_pipeline(data_path: str):\n",
    "    \"\"\"Orquesta el pipeline completo de entrenamiento y evaluación.\"\"\"\n",
    "    \n",
    "    df = load_raw_data(data_path)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train, y_train, preprocessor = preprocess_and_split(df_train, 'Churn')\n",
    "    print(\"Datos de entrenamiento preprocesados y listos.\")\n",
    "    \n",
    "    scoring_metric, initial_params_grid, balancing_techniques = detect_class_imbalance(y_train)\n",
    "\n",
    "    best_overall_score = -1\n",
    "    best_overall_model = None\n",
    "    best_overall_technique = ''\n",
    "    \n",
    "    for tech_name, sampler in balancing_techniques.items():\n",
    "        print(f\"\\n--- Probando la técnica: {tech_name} ---\")\n",
    "\n",
    "        steps = [('preprocessor', preprocessor)]\n",
    "        if sampler:\n",
    "            steps.append(('sampler', sampler))\n",
    "            \n",
    "        steps.append(('model', ModelFactory.create_model_base('xgboost')))\n",
    "        model_pipeline = Pipeline(steps=steps)\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model_pipeline,\n",
    "            param_grid=initial_params_grid,\n",
    "            cv=5,\n",
    "            scoring=scoring_metric,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"\\nMejor {scoring_metric} para {tech_name}: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "\n",
    "        if grid_search.best_score_ > best_overall_score:\n",
    "            best_overall_score = grid_search.best_score_\n",
    "            best_overall_model = grid_search.best_estimator_\n",
    "            best_overall_technique = tech_name\n",
    "\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(f\"Mejor modelo global encontrado con la técnica: {best_overall_technique}\")\n",
    "    print(f\"Mejor {scoring_metric} global: {best_overall_score:.4f}\")\n",
    "    print(\"=======================================================\")\n",
    "\n",
    "    models_dir = 'models'\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    \n",
    "    joblib.dump(best_overall_model, os.path.join(models_dir, 'best_overall_model.pkl'))\n",
    "    \n",
    "    X_test, y_test, _ = preprocess_and_split(df_test, 'Churn')\n",
    "    y_pred = best_overall_model.predict(X_test)\n",
    "    evaluate_model(y_test, y_pred, 'XGBoost (Optimizado)')\n",
    "    \n",
    "    print(\"\\nMejor modelo y preprocesador guardados en la carpeta 'models'.\")\n",
    "    print(\"\\nPipeline de entrenamiento completado.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = 'data/raw/Telco-Customer-Churn.csv'\n",
    "    run_training_pipeline(DATA_PATH)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
